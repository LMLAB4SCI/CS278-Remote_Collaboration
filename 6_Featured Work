In this section, we introduce the five accepted papers in this
issue and briefly summarize their results and findings, in the
context of the three research aspects; communication cues
and multimodalities, view sharing and situational awareness,
and human factors.
The combination of visual communication cues in mixed
reality remote collaboration In this paper (Kim et al. [26]),
explores the impact of combining different visual cues on
the user-perceived quality of communication and collaboration
performance. The visual cues were a pointer, sketches,
and hand gesture cues, which are traditionally independent
in remote collaboration.
Two conducted user studies compared four combinations
of the three cues: (1) hand only, (2) hand+pointer, (3)
hand+sketch, and (4) hand+pointer+sketch, while varying
the sharing view mode, either a dependent or an independent
view. In the dependent view mode, in which the users shares
the same view, participants mostly used the hand gestures
due to ease of use and intuitiveness, while the sketches were
also useful for clarifying any misunderstandings between the
users. Interestingly, in the independent view mode, in which
the users had their own views, the hand gesture cues were
perceived difficult to identify where the hand referred to,
because of the different perspectives of the users. However,
the pointer cue turned out to be very useful as an alternative
for the hand gestures.
The effects of spatial auditory and visual cues on mixed reality
remote collaboration: Despite the importance of auditory
cues in remote collaboration, the current state of research
in MR remote collaboration largely focuses on investigating
the effects of visual cues. To fill the gap, this paper (Yang et
al. [47]), presents a multimodal system that can provide both
spatial auditory and visual cues to the users, and investigates
the effects of such multimodal cues on task performance and
perception in an indoor visual search task through. The user
study results show that the remote user’s spatialized voice
guidance and auditory beacons could improve the local user’s
performance to find objects in a highly cluttered environment
by providing auditory clues for spatial directions. The paper
further discusses the potential implications in the integration
of spatial auditory and visual cues for better user experience
and performance in remote collaboration contexts.
Sharing gaze rays for visual target identification tasks in
collaborative augmented reality: Sharing social cues among
collaborators is a powerful strategy to improve the quality of
communication. In this paper (Erickson et al. [7]), authors
explore the use of a shared gaze ray in a target identification
task while investigating the influence of different gaze errors
in the task performance, such as response time and error rate.
The results show that different error levels in the shared
gaze had strong effects on participants’ performance, while
the distance to the task object had less influence on the
performance and user experience. Interestingly, the study
also reveals that participants’ self-assessed performance was
lower than the actual performance in the target identification
task.
Exploring interaction techniques for 360 panoramas inside
a 3D reconstructed scene for mixed reality remote collaboration:
In this paper (Teo et al. [45]), authors investigate
the effects of different ways to show 360-degree live video
on the user’s sense of social/co-presence and cognitive load
in collaborative object moving tasks. The conducted user
study compared two modes: (1) projecting the 360-degree
live video in a sphere, called the “photo-bubble” mode, and
(2) projecting it as a texture on a low-resolution 3D reconstructed
mesh, called the “projective texture ” mode. The
paper presents the results suggesting that both modes could
provide a high level of social/co-presence and reduce cognitive
load, and discusses the advantages and limitations of
each method.
Effects of personality traits on user trust in human–machine
collaborations:Beyond collaborations between human users,
there is increasing research in human–machine (or human–
agent) collaborations, considering the convergence of
advanced artificial intelligence (AI), such as intelligent virtual
assistants, and immersive AR/VR technologies [39].
This paper Zhou et al. [48] explores the effect of a user’s
personality trait on the level of trust in a machine’s decisions
when collaborating with the machine. The user study
conducted in the paper varied the task design in terms of
the levels of uncertainty and cognitive load, and examined
how participants with different personality traits perceived
the trustworthiness of the collaborative machine’s decisions
while performing the task. The results reveals that the users’
personality traits affect their perceived trust in machine decision
differently with respect to the uncertainty and cognitive
load conditions.
